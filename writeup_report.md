# **Behavioral Cloning Project**
Peter Moran /
July 2017 /
Udacity Self-Driving Car Nanodegree

**Project Goals**

* Build, a convolution neural network in Keras that predicts steering angles from images for end-to-end driving.
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./examples/center.jpg "Center Sample Image"
[image2]: ./examples/left.jpg "Left Side Sample Image"

## Rubric Points
Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.

### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* `model.py` containing the script to create and train the model.
* `simulator_reader.py` containing code used for loading in CSV data generated by the simulator.
* `img_inspector.py` used to playback the simulation data.
* `drive.py` for driving the car in autonomous mode.
* `model.h5` containing a trained convolution neural network.
* `writeup_report.md` summarizing the results.
* `video.mp4` a video of the car vehicle driving autonomously one lap around track 1.

#### 2. Submission includes functional code
Using the Udacity provided simulator and my `drive.py` file, the car can be driven autonomously around the track by executing
```sh
python drive.py model.h5
```

#### 3. Submission code is usable and readable

The `model.py` file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

My model consists of three series of 5x5 convolutions with depths between 64 and 256, each separated by 3x3
max-pooling layers. These convolutional layers were followed by two fully connected layers and the
output layer.

**Additional notes**
* The images are cropped and normalized by the model using a Keras lambda layer.
* The model includes RELU layers to introduce nonlinearity.
* Dropout, L2 Normalization, and Batch Normalization can all be optionally applied via `create_model()`'s
 parameters, however empirical testing showed that these methods reduced performance and were ultimately
 not used.

Further discussion can be found in the "Final Model Architecture" section.

#### 2. Attempts to reduce overfitting in the model

The model was trained and validated on different data sets to ensure that the model was not overfitting.
The model was tested by running it through the simulator and ensuring that the vehicle could stay on the
track.

Ultimately, overfitting was not a problem. Despite testing a wealth of normalization tactics (Dropout, L2
Normalization, and Batch Normalization), all of which can easily be re-enabled, the best validation and
driving performance came from networks that did not use these techniques.

#### 3. Model parameter tuning

Multiple parameters, some unique to this implementation, were empirically tuned for greater performance.

* `SIDECAM_OFFSET` defines the steering angle magnitude to be added to simulator samples when
using side camera images as if they were captured by the center camera.
* `DROPOUT` defined the dropout rate to use on every layer. Set to `None` if
dropout was not desired.
* `L2_WEIGHT` defined the scaling term to be applied to L2 Normalization. Set to `None` if L2 Normalization
not desired.
* `BATCH_NORM` whether to apply batch normalization.

The model used an Adam Optimizer, so the learning rate was not tuned manually.

#### 4. Appropriate training data

A combination of simulation collected samples and augmented data were used to form the training and
validation datasets. The training set used 19,740 samples, using both simulation samples and augmented
data. The validation set used and 3,290 samples, and only used samples from the simlator (ie no
augmented data).

Early termination and checkpointing were used to ensure training stopped after
validation loss stopped improving, and so that the model with the best validation
accuracy was saved and not overwritten.

For details about how I created the training data, see the next section. 

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

This project was more about data collection than building models, which is to say, I spent way
more time working on ways to collect better data than changing the architecture of my model. I saw
far greater performance improvement due to better data than I saw with trying different
architectures. I was very careful in collecting data, using smooth analog inputs and removing
any moments of bad driving from my data sets.

Nearly my first attempt used a simple, but large network inspired by VGG,  paired down to work
on my 2GB graphics card. This resulted in large convolutions separated by max pooling. To my luck,
the model was very quickly successful and so I did not need to change the structure much other than
tuning kernel sizes and attempting normalization methods. The model as implemented now is largely
the one I started out with.

Rather than changing my model to improve performance, I improved my data. The first tests on the
system used center car images alone, but this did not get much further than the first turn. By
augmenting with both flipped images and using the side camera images with steering offsets,
as if they were center images, I was instantly able to get my car to drive around the track.

For a few parts of the track, the car drove a little less stable than I preferred. I simply
drove these parts again and added it to my dataset and the problems were solved.

I also got the car to drive on the second track. Before I had a training set that included
data from it, the car was not able to drive it. Once I added samples from driving the second track,
however, it drove it without issue.

One final issue was how the car oscillated as it drove, swerving back and forth while still remaining on the track.
While technically passing, I was able to reduce this problem by lowering the steering offset for the side cameras.

At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

#### 2. Final Model Architecture

The final model architecture, found in `create_model()` in `model.py`, consisted of a convolution
neural network with the following layers and layer sizes, resulting in a model with
3,979,137 training parameters.

Though abnormal, the network uses 3x3 max pooling in order to keep the model
small enough to run on a 2GB graphics card. Performance was still great despite
the potential loss of information in these layers.

The final layer (before flattening and the fully connected layer) had feature maps
with dimensions (2, 11). The reasoning for this, and for not making it even smaller,
was that I wanted to retain some locality of my data. Convolution is great for
being location independent, but in this case I felt that position of features would
be important to the system, and thus I wanted to leave this information available to
the network. Thus, the final layer slightly larger than in other implementations.
For example, Nvidia's final layer before being fully connected has only a single row,
while this network uses two.


**Model Summary**
```
Layer (type)                 Output Shape              Param #
=================================================================
lambda_1 (Lambda)            (None, 160, 320, 3)       0
_________________________________________________________________
cropping2d_1 (Cropping2D)    (None, 65, 320, 3)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 65, 320, 64)       4864
_________________________________________________________________
activation_1 (Activation)    (None, 65, 320, 64)       0
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 21, 106, 64)       0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 21, 106, 128)      204928
_________________________________________________________________
activation_2 (Activation)    (None, 21, 106, 128)      0
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 35, 128)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 7, 35, 256)        819456
_________________________________________________________________
activation_3 (Activation)    (None, 7, 35, 256)        0
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 11, 256)        0
_________________________________________________________________
flatten_1 (Flatten)          (None, 5632)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               2884096
_________________________________________________________________
activation_4 (Activation)    (None, 512)               0
_________________________________________________________________
dense_2 (Dense)              (None, 128)               65664
_________________________________________________________________
activation_5 (Activation)    (None, 128)               0
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 129
=================================================================
```

#### 3. Creation of the Training Set & Training Process

For all sampling, I used a video game controller for precise control of the driving. This allowed
me to make smooth turns and have steering data that accurately reflects the ideal driving angle
at any given moment.

I used only four sets of data for this system.
1. Track 1 forwards
2. Track 1 backwards
3. Track 1 trouble spots
4. Track 2 forwards

The training set was also augmented with flipped images from every single
sample in every single session, as well as every left and right side camera
image. Thus 25% of the training data was raw sampled images, 25% were flipped
versions of those images, and 50% were the side camera images used as center
images. While I could have tuned these distributions, such measures were
not needed. All augmented data was generated on the fly by a generator.

All images were also normalized to have a zero mean with pixel values
ranging from `-0.5` to `0.5`.

In all I had 8,225 raw samples from driving in the simulator. The training set used
60% of this data (4,935 samples), but augmented it, resulting in 4 times more samples
for a total of 19,740 samples used for training. Validation used the other 40% of the
simulation samples, without augmentation, resulting in a total of 3,290 validation
samples. Randomization was used in both splitting and ordering the datasets.

**Examples**

Center Camera

![alt text][image1]

Side Camera

![alt text][image2]